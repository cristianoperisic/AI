# -*- coding: utf-8 -*-
# =================================================================================================
# ìµœì¢… ë²„ì „ v5: ìƒì„¸ ì£¼ì„ ì¶”ê°€ ë²„ì „
# -------------------------------------------------------------------------------------------------
# [í”„ë¡œì íŠ¸ ëª©í‘œ]
#  - ê³¼ê±° ë§¤ì¶œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ê° ìƒì ì˜ ë©”ë‰´ë³„ 'ë¯¸ëž˜ 7ì¼ê°„ì˜ ë§¤ì¶œ ìˆ˜ëŸ‰'ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
#
# [í•µì‹¬ ì „ëžµ]
#  1. ë°ì´í„° ì •ì œ: 'ì´ìƒì¹˜ ì²˜ë¦¬'ë¥¼ í†µí•´ ëª¨ë¸ í•™ìŠµì„ ë°©í•´í•˜ëŠ” ë¹„ì •ìƒì ì¸ ë°ì´í„°ë¥¼ ì•ˆì •í™”ì‹œí‚µë‹ˆë‹¤.
#  2. íŠ¹ì§• ê³µí•™: ë‚ ì§œ, Lag(ê³¼ê±° ê°’), Rolling(ì´ë™ í†µê³„) ë“± ë‹¤ì–‘í•œ íŠ¹ì§•ì„ ìƒì„±í•˜ì—¬ ëª¨ë¸ì—ê²Œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
#  3. ëª¨ë¸ ìµœì í™”: 'Optuna'ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì— ê°€ìž¥ ì í•©í•œ XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ìžë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
#  4. ë¯¸ëž˜ ì˜ˆì¸¡: 'ìž¬ê·€ ì˜ˆì¸¡' ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬, ì˜ˆì¸¡í•œ ê°’ì„ ë‹¤ì‹œ ìž…ë ¥ìœ¼ë¡œ í™œìš©í•˜ë©° 7ì¼ í›„ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
# =================================================================================================

# ===== 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° =====
# ë°ì´í„° ë¶„ì„ê³¼ ëª¨ë¸ë§ì— í•„ìš”í•œ ë„êµ¬(ë¼ì´ë¸ŒëŸ¬ë¦¬)ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

import warnings

warnings.filterwarnings(
    "ignore", category=FutureWarning
)  # ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ë¥¼ ìˆ¨ê²¨ì„œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ê¹”ë”í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.

import pandas as pd  # ë°ì´í„°ë¥¼ í‘œ(DataFrame) í˜•íƒœë¡œ ë‹¤ë£¨ëŠ” ë°ì´í„° ë¶„ì„ì˜ í•µì‹¬ ë„êµ¬
import numpy as np  # ë³µìž¡í•œ ìˆ˜ì¹˜ ë° ë°°ì—´ ê³„ì‚°ì„ ìœ„í•œ ë„êµ¬
from pathlib import (
    Path,
)  # ìš´ì˜ì²´ì œ(OS)ì— ìƒê´€ì—†ì´ íŒŒì¼ ê²½ë¡œë¥¼ ì•ˆì •ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ìœ„í•œ ë„êµ¬
from sklearn.preprocessing import (
    LabelEncoder,
)  # ë¬¸ìžì—´ ë°ì´í„°ë¥¼ ìˆ«ìž IDë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬
from sklearn.model_selection import (
    TimeSeriesSplit,
)  # ì‹œê³„ì—´ ë°ì´í„°ì— íŠ¹í™”ëœ êµì°¨ ê²€ì¦ ë„êµ¬
import xgboost as xgb  # ì´ í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ì˜ˆì¸¡ ëª¨ë¸
import optuna  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìžë™ íŠœë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ.")

# ===== 2. ê²½ë¡œ ì„¤ì • =====
# ë¶„ì„ì— í•„ìš”í•œ ë°ì´í„° íŒŒì¼ë“¤ì˜ ìœ„ì¹˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.

# ì‚¬ìš©ìžì˜ ì›ëž˜ ì½”ë“œ ê²½ë¡œ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
BASE_DIR = Path(r"C:\Users\lw105\OneDrive\ë°”íƒ• í™”ë©´\open")
TRAIN_FP = (
    BASE_DIR / "train" / "train.csv"
)  # 'open' í´ë” ì•ˆì˜ 'train' í´ë”ì— ìžˆëŠ” íŒŒì¼ì„ ì§ì ‘ ì§€ì •
TEST_DIR = BASE_DIR / "test"
SAMPLE_FP = BASE_DIR / "sample_submission.csv"

# ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ì‚¬ì „ì— ë°©ì§€í•©ë‹ˆë‹¤.
if not TRAIN_FP.exists() or not TEST_DIR.exists() or not SAMPLE_FP.exists():
    print(
        "ðŸš¨ ê²½ë¡œ ì˜¤ë¥˜: 'open/train', 'test' í´ë” ë° 'sample_submission.csv' íŒŒì¼ì´ ì§€ì •ëœ ê²½ë¡œì— ìžˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
    )
    print(f"ì˜ˆìƒ í•™ìŠµ ë°ì´í„° ê²½ë¡œ: {TRAIN_FP}")
    exit()  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
else:
    print("íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ.")

# ===== 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ =====
print("ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì‹œìž‘...")
# 'Long' í¬ë§·(ì„¸ë¡œë¡œ ê¸´ í˜•íƒœ)ì˜ train.csvë¥¼ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤.
train = pd.read_csv(TRAIN_FP)
train["ì˜ì—…ì¼ìž"] = pd.to_datetime(
    train["ì˜ì—…ì¼ìž"]
)  # ë‚ ì§œ ê´€ë ¨ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ë°ì´í„° íƒ€ìž…ì„ ë³€í™˜í•©ë‹ˆë‹¤.

# 3.1. ì´ìƒì¹˜ ì²˜ë¦¬ (IQR ê¸°ë°˜)
# "ìƒì‹ì ì¸ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°(ì´ìƒì¹˜)ëŠ” ëª¨ë¸ í•™ìŠµì— ë°©í•´ê°€ ë˜ë¯€ë¡œ, ì•ˆì •ì ì¸ ê°’ìœ¼ë¡œ ë°”ê¿”ì£¼ìž"
print("ì´ìƒì¹˜ ì²˜ë¦¬ ì‹œìž‘...")


def handle_outliers_iqr(df_group):
    # ë§¤ì¶œì´ 0ì¸ ê²½ìš°ê°€ ë§Žì„ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ, 0ì„ ì œì™¸í•˜ê³  ë¶„ìœ„ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë” í˜„ì‹¤ì ì¸ ì´ìƒì¹˜ ë²”ìœ„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    non_zero_sales = df_group[df_group["ë§¤ì¶œìˆ˜ëŸ‰"] > 0]["ë§¤ì¶œìˆ˜ëŸ‰"]
    if len(non_zero_sales) < 5:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì´ìƒì¹˜ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.
        return df_group

    # ë°ì´í„°ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ 1ì‚¬ë¶„ìœ„ìˆ˜(Q1)ì™€ 3ì‚¬ë¶„ìœ„ìˆ˜(Q3)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    q1, q3 = non_zero_sales.quantile(0.25), non_zero_sales.quantile(0.75)
    iqr = q3 - q1  # Q1ê³¼ Q3 ì‚¬ì´ì˜ ë²”ìœ„(IQR)

    # ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤. (í†µê³„ì ìœ¼ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” 1.5ë°°ìˆ˜ ê¸°ì¤€)
    lower_bound = max(
        0, q1 - 1.5 * iqr
    )  # í•˜í•œì„  (ë§¤ì¶œì´ ìŒìˆ˜ì¼ ìˆ˜ëŠ” ì—†ìœ¼ë¯€ë¡œ 0ë³´ë‹¤ ìž‘ì•„ì§€ì§€ ì•Šê²Œ í•¨)
    upper_bound = q3 + 1.5 * iqr  # ìƒí•œì„ 

    # ì´ìƒì¹˜ë¥¼ ì •ìƒ ë²”ìœ„ì˜ ìµœëŒ€/ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´(Clipping)í•©ë‹ˆë‹¤.
    df_group["ë§¤ì¶œìˆ˜ëŸ‰"] = np.clip(df_group["ë§¤ì¶œìˆ˜ëŸ‰"], lower_bound, upper_bound)
    return df_group


# ê° ë©”ë‰´ë³„ë¡œ ê·¸ë£¹ì„ ì§€ì–´ ì´ìƒì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
train = train.groupby("ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…", group_keys=False).apply(handle_outliers_iqr)
print("ì´ìƒì¹˜ ì²˜ë¦¬ ì™„ë£Œ.")

# 3.2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
sample = pd.read_csv(SAMPLE_FP)
tests = {}
for i in range(10):
    name = f"TEST_{i:02d}"
    df = pd.read_csv(TEST_DIR / f"{name}.csv")
    df["ì˜ì—…ì¼ìž"] = pd.to_datetime(df["ì˜ì—…ì¼ìž"])
    tests[name] = df

# ===== 4. íŠ¹ì§• ê³µí•™ (Feature Engineering) =====
# "ëª¨ë¸ì´ ë” ë˜‘ë˜‘í•˜ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìžˆë„ë¡, ì›ë³¸ ë°ì´í„°ë¡œë¶€í„° ìœ ìš©í•œ ížŒíŠ¸(íŠ¹ì§•)ë“¤ì„ ë§Œë“¤ì–´ì£¼ìž"
print("íŠ¹ì§• ê³µí•™ ì‹œìž‘...")

# 4.1. ë¼ë²¨ ì¸ì½”ë”©
le = LabelEncoder()
train["item_id"] = le.fit_transform(
    train["ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…"]
)  # ë©”ë‰´ ì´ë¦„ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìžˆëŠ” ê³ ìœ  ìˆ«ìž IDë¡œ ë³€í™˜


# 4.2. ë‚ ì§œ íŠ¹ì§• ìƒì„± í•¨ìˆ˜
def make_date_feats(df):
    out = df.copy()
    # ê¸°ë³¸ ë‚ ì§œ ì •ë³´
    out["year"], out["month"], out["day"], out["weekday"] = (
        out["ì˜ì—…ì¼ìž"].dt.year,
        out["ì˜ì—…ì¼ìž"].dt.month,
        out["ì˜ì—…ì¼ìž"].dt.day,
        out["ì˜ì—…ì¼ìž"].dt.weekday,
    )
    out["is_weekend"] = (
        out["weekday"].isin([5, 6]).astype(int)
    )  # ì£¼ë§ ì—¬ë¶€ (í† =5, ì¼=6)

    # ì£¼ê¸°ì„± íŠ¹ì§•: 12ì›”ê³¼ 1ì›”ì´ ê°€ê¹ë‹¤ëŠ” ê²ƒì„ ëª¨ë¸ì—ê²Œ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ì‹œê³„ì²˜ëŸ¼ ì›í˜•ìœ¼ë¡œ ë³€í™˜
    out["month_sin"], out["month_cos"] = np.sin(
        2 * np.pi * out["month"] / 12.0
    ), np.cos(2 * np.pi * out["month"] / 12.0)
    out["wday_sin"], out["wday_cos"] = np.sin(2 * np.pi * out["weekday"] / 7.0), np.cos(
        2 * np.pi * out["weekday"] / 7.0
    )
    return out


train = make_date_feats(train)
train = train.sort_values(
    ["item_id", "ì˜ì—…ì¼ìž"]
)  # Lag, Rolling ê³„ì‚°ì„ ìœ„í•´ ì•„ì´í…œë³„, ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬

# 4.3. Lag & Rolling íŠ¹ì§• ìƒì„±
# Lag: "ì–´ì œëŠ” ëª‡ ê°œ íŒ”ë ¸ë‚˜?", "ì§€ë‚œì£¼ ê°™ì€ ìš”ì¼ì—ëŠ” ëª‡ ê°œ íŒ”ë ¸ë‚˜?"
for lag in [1, 7, 14, 28]:
    train[f"lag{lag}"] = train.groupby("item_id")["ë§¤ì¶œìˆ˜ëŸ‰"].shift(lag)

# Rolling: "ì§€ë‚œ 7ì¼ê°„ì˜ í‰ê·  ë§¤ì¶œì€?", "ë§¤ì¶œ ë³€ë™ì„±ì€ ì–´ë• ë‚˜?" (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ shift(1) ì ìš©)
g = train.groupby("item_id")["ë§¤ì¶œìˆ˜ëŸ‰"]
train["roll7_mean"], train["roll14_mean"], train["roll7_std"] = (
    g.shift(1).rolling(7).mean(),
    g.shift(1).rolling(14).mean(),
    g.shift(1).rolling(7).std(),
)

# íŠ¹ì§• ìƒì„± ê³¼ì •ì—ì„œ ìƒê¸´ ê²°ì¸¡ì¹˜(NaN)ê°€ ìžˆëŠ” í–‰ì€ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì œê±°
train = train.dropna()
print("íŠ¹ì§• ê³µí•™ ì™„ë£Œ.")

# 4.4. ìµœì¢… í•™ìŠµ ë°ì´í„° ì¤€ë¹„
feature_cols = [
    "year",
    "month",
    "day",
    "weekday",
    "is_weekend",
    "month_sin",
    "month_cos",
    "wday_sin",
    "wday_cos",
    "item_id",
    "lag1",
    "lag7",
    "lag14",
    "lag28",
    "roll7_mean",
    "roll14_mean",
    "roll7_std",
]
X, y = train[feature_cols], train["ë§¤ì¶œìˆ˜ëŸ‰"].astype(
    float
)  # X: ë¬¸ì œì§€(íŠ¹ì§•), y: ì •ë‹µì§€(ë§¤ì¶œìˆ˜ëŸ‰)

# ===== 5. Optunaë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ =====
# "ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœëŒ€ë¡œ ëŒì–´ì˜¬ë¦¬ê¸° ìœ„í•´, ìµœì ì˜ ì„¤ì •ê°’ì„ ìžë™ìœ¼ë¡œ ì°¾ì•„ë³´ìž"
print("Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìž‘...")
try:
    import torch

    HAS_CUDA = torch.cuda.is_available()
except:
    HAS_CUDA = False

# TimeSeriesSplit ê°ì²´ë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ì—¬ objective í•¨ìˆ˜ì™€ ìµœì¢… í•™ìŠµì—ì„œ ê³µìœ 
tscv = TimeSeriesSplit(n_splits=5)


# Optunaê°€ ìµœì í™”í•  ëª©í‘œ(Objective) í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ì´ í•¨ìˆ˜ëŠ” íŠ¹ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ê·¸ ì„±ëŠ¥(RMSE)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
def objective(trial):
    # íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„(Search Space)ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    params = {
        "objective": "reg:squarederror",
        "eval_metric": "rmse",
        "tree_method": "hist",
        "device": "cuda" if HAS_CUDA else "cpu",
        "seed": 42,
        # trial.suggest_... : Optunaê°€ ì´ ë²”ìœ„ ë‚´ì—ì„œ ë‹¤ìŒ ì‹œë„í•´ë³¼ ê°’ì„ 'ì œì•ˆ'í•©ë‹ˆë‹¤.
        "max_depth": trial.suggest_int("max_depth", 4, 12),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "gamma": trial.suggest_float("gamma", 1e-8, 1.0, log=True),
        "lambda": trial.suggest_float("lambda", 1e-8, 1.0, log=True),
        "alpha": trial.suggest_float("alpha", 1e-8, 1.0, log=True),
    }
    rmses = []
    # êµì°¨ ê²€ì¦ì„ í†µí•´ íŒŒë¼ë¯¸í„° ì¡°í•©ì˜ ì„±ëŠ¥ì„ ì•ˆì •ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    for tr_idx, va_idx in tscv.split(X):
        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]
        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]
        dtr, dva = xgb.DMatrix(X_tr, label=y_tr), xgb.DMatrix(X_va, label=y_va)

        # Pruning Callback: ì„±ëŠ¥ì´ ë‚˜ì  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì‹œë„ë¥¼ ì¡°ê¸°ì— ì¤‘ë‹¨ì‹œì¼œ íƒìƒ‰ ì‹œê°„ì„ ì ˆì•½í•©ë‹ˆë‹¤.
        pruning_callback = optuna.integration.XGBoostPruningCallback(trial, "val-rmse")
        booster = xgb.train(
            params,
            dtr,
            num_boost_round=1000,
            evals=[(dva, "val")],
            early_stopping_rounds=50,
            callbacks=[pruning_callback],
            verbose_eval=False,
        )
        rmses.append(booster.best_score)

    # êµì°¨ ê²€ì¦ ê²°ê³¼ì˜ í‰ê·  RMSEë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. OptunaëŠ” ì´ ê°’ì„ 'ìµœì†Œí™”'í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.
    return np.mean(rmses)


# Optuna ìŠ¤í„°ë””(íƒìƒ‰ ê³¼ì •)ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
study = optuna.create_study(
    direction="minimize", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
)
study.optimize(
    objective, n_trials=50
)  # 50ë²ˆì˜ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ìµœì í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.

print("íŠœë‹ ì™„ë£Œ!")
print(f"ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {study.best_params}")
print(f"ìµœì  RMSE: {study.best_value}")

# ===== 6. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ =====
print("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œìž‘...")
best_params = study.best_params
best_params.update(
    {
        "objective": "reg:squarederror",
        "eval_metric": "rmse",
        "tree_method": "hist",
        "device": "cuda" if HAS_CUDA else "cpu",
    }
)

# ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì  í•™ìŠµ íšŸìˆ˜(best_iteration)ë¥¼ ë‹¤ì‹œ ì°¾ìŠµë‹ˆë‹¤.
last_tr_idx, last_va_idx = list(tscv.split(X))[-1]
X_tr, X_va = X.iloc[last_tr_idx], X.iloc[last_va_idx]
y_tr, y_va = y.iloc[last_tr_idx], y.iloc[last_va_idx]
dtr, dva = xgb.DMatrix(X_tr, label=y_tr), xgb.DMatrix(X_va, label=y_va)
booster = xgb.train(
    best_params,
    dtr,
    num_boost_round=5000,
    evals=[(dva, "val")],
    early_stopping_rounds=100,
    verbose_eval=False,
)
best_iter = booster.best_iteration
print(f"ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì°¾ì€ í•™ìŠµ íšŸìˆ˜: {best_iter}")

# ëª¨ë“  í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.
dall = xgb.DMatrix(X, label=y)
final_model = xgb.train(
    best_params, dall, num_boost_round=best_iter, verbose_eval=False
)
print("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

# ===== 7. ìž¬ê·€ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± =====
print("ìž¬ê·€ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± ì‹œìž‘...")
all_preds = []
full_history = train.copy()  # ìž¬ê·€ ì˜ˆì¸¡ì— ì‚¬ìš©í•  ì „ì²´ ê³¼ê±° ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë³µì‚¬í•´ ë‘¡ë‹ˆë‹¤.

for test_name, test_df in tests.items():
    test_df = test_df.copy()
    test_df["item_id"] = le.transform(test_df["ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…"])
    test_df = make_date_feats(test_df)

    # ì˜ˆì¸¡ì˜ ê¸°ë°˜ì´ ë  ê³¼ê±° ë°ì´í„°ë¥¼ ë§¤ë²ˆ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤. (í•™ìŠµ ë°ì´í„° + í•´ë‹¹ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
    history = pd.concat([full_history, test_df], ignore_index=True)
    history = history.sort_values(["item_id", "ì˜ì—…ì¼ìž"])

    last_date = test_df["ì˜ì—…ì¼ìž"].max()
    items = test_df["ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…"].unique()

    preds_rows = []
    current_date = last_date
    for step in range(1, 8):  # 7ì¼ê°„ í•˜ë£¨ì”© ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
        target_date = current_date + pd.Timedelta(days=1)

        # 1. ì˜ˆì¸¡í•  ë‚ ì§œì˜ ê¸°ë³¸ í”„ë ˆìž„(ë¼ˆëŒ€) ìƒì„±
        frame = pd.DataFrame(
            {"ì˜ì—…ì¼ìž": np.repeat(target_date, len(items)), "ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…": items}
        )
        frame["item_id"] = le.transform(frame["ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…"])
        frame = make_date_feats(frame)

        # 2. ì—…ë°ì´íŠ¸ëœ 'history'ë¥¼ ì‚¬ìš©í•˜ì—¬ Lag & Rolling íŠ¹ì§• ê³„ì‚°
        temp_hist = history.copy()
        for lag in [1, 7, 14, 28]:
            lagged = temp_hist[["ì˜ì—…ì¼ìž", "item_id", "ë§¤ì¶œìˆ˜ëŸ‰"]].copy()
            lagged["ì˜ì—…ì¼ìž"] = lagged["ì˜ì—…ì¼ìž"] + pd.Timedelta(days=lag)
            frame = frame.merge(
                lagged.rename(columns={"ë§¤ì¶œìˆ˜ëŸ‰": f"lag{lag}"}),
                on=["ì˜ì—…ì¼ìž", "item_id"],
                how="left",
            )

        roll_base = temp_hist.sort_values(["item_id", "ì˜ì—…ì¼ìž"]).copy()
        gb = roll_base.groupby("item_id")["ë§¤ì¶œìˆ˜ëŸ‰"]
        roll_base["roll7_mean"] = gb.rolling(7).mean().reset_index(0, drop=True)
        roll_base["roll14_mean"] = gb.rolling(14).mean().reset_index(0, drop=True)
        roll_base["roll7_std"] = gb.rolling(7).std().reset_index(0, drop=True)
        roll_base["ì˜ì—…ì¼ìž"] = roll_base["ì˜ì—…ì¼ìž"] + pd.Timedelta(days=1)
        frame = frame.merge(
            roll_base[
                ["ì˜ì—…ì¼ìž", "item_id", "roll7_mean", "roll14_mean", "roll7_std"]
            ],
            on=["ì˜ì—…ì¼ìž", "item_id"],
            how="left",
        )

        frame[feature_cols] = frame[feature_cols].fillna(0)  # ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€

        # 3. ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        X_pred = frame[feature_cols]
        dpred = xgb.DMatrix(X_pred)
        yhat = final_model.predict(dpred)
        yhat = np.clip(yhat, 0, None)  # ë§¤ì¶œì´ ìŒìˆ˜ê°€ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ 0ìœ¼ë¡œ ì¡°ì •
        frame["pred"] = yhat

        # 4. ì˜ˆì¸¡ê°’ì„ historyì— ì¶”ê°€í•˜ì—¬ ë‹¤ìŒ ë‚  ì˜ˆì¸¡ì— ì‚¬ìš© (ìž¬ê·€ì˜ í•µì‹¬)
        add_hist = frame[["ì˜ì—…ì¼ìž", "item_id", "ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…", "pred"]].rename(
            columns={"pred": "ë§¤ì¶œìˆ˜ëŸ‰"}
        )
        history = pd.concat([history, add_hist], ignore_index=True)

        # 5. ìµœì¢… ì œì¶œìš©ìœ¼ë¡œ ê²°ê³¼ ì €ìž¥
        frame_out = frame[["ì˜ì—…ì¼ìž", "ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…", "pred"]].copy()
        frame_out["ì˜ì—…ì¼ìž"] = f"{test_name}+{step}ì¼"
        preds_rows.append(frame_out)

        current_date = target_date  # ê¸°ì¤€ ë‚ ì§œë¥¼ í•˜ë£¨ ë’¤ë¡œ ì—…ë°ì´íŠ¸

    test_pred = pd.concat(preds_rows, ignore_index=True)
    wide = test_pred.pivot(index="ì˜ì—…ì¼ìž", columns="ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…", values="pred")
    all_preds.append(wide)

# ===== 8. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± =====
submission = pd.concat(all_preds)
submission = submission.reset_index().rename(columns={"index": "ì˜ì—…ì¼ìž"})
submission = submission[sample.columns]  # ì œì¶œ ìƒ˜í”Œê³¼ ì—´ ìˆœì„œ/ì´ë¦„ì„ ì •í™•ížˆ ì¼ì¹˜ì‹œí‚´
out_path = BASE_DIR / "submission_final_tuned_v5_detailed_comments.csv"
submission.to_csv(out_path, index=False, encoding="utf-8-sig")

print(f"âœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ìž¥ ì™„ë£Œ: {out_path}")
