# -*- coding: utf-8 -*-
# =================================================================================================
# LightGBM + CatBoost ì•™ìƒë¸” ëª¨ë¸
# -------------------------------------------------------------------------------------------------
# [í•µì‹¬ ì „ëžµ]
#  1. ë°ì´í„° ì²˜ë¦¬: ì´ìƒì¹˜ ì²˜ë¦¬ ë° íŠ¹ì§• ê³µí•™ì€ ê¸°ì¡´ XGBoost ì½”ë“œì™€ ë™ì¼í•œ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
#  2. ê°œë³„ ëª¨ë¸ í•™ìŠµ: ì†ë„ì˜ LightGBMê³¼ ì•ˆì •ì„±ì˜ CatBoostë¥¼ ê°ê° ìµœì ì˜ ìƒíƒœë¡œ í•™ìŠµ
#  3. ì‹¤ì‹œê°„ ì•™ìƒë¸”: ìž¬ê·€ ì˜ˆì¸¡ ì‹œ, ê° ë‚ ì§œë§ˆë‹¤ ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· ë‚´ì–´ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©
# =================================================================================================

# ===== 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° =====
import warnings

warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import TimeSeriesSplit
import lightgbm as lgb
import catboost as cb

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ.")

# ===== 2. ê²½ë¡œ ì„¤ì • =====
BASE_DIR = Path(r"C:\Users\lw105\OneDrive\ë°”íƒ• í™”ë©´\open")
TRAIN_FP = BASE_DIR / "train" / "train.csv"
TEST_DIR = BASE_DIR / "test"
SAMPLE_FP = BASE_DIR / "sample_submission.csv"

if not TRAIN_FP.exists():
    print(f"ðŸš¨ ê²½ë¡œ ì˜¤ë¥˜: {TRAIN_FP} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()
else:
    print("íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ.")

# ===== 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ =====
print("ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì‹œìž‘...")
train = pd.read_csv(TRAIN_FP)
train["ì˜ì—…ì¼ìž"] = pd.to_datetime(train["ì˜ì—…ì¼ìž"])


# 3.1. ì´ìƒì¹˜ ì²˜ë¦¬
def handle_outliers_iqr(df_group):
    non_zero_sales = df_group[df_group["ë§¤ì¶œìˆ˜ëŸ‰"] > 0]["ë§¤ì¶œìˆ˜ëŸ‰"]
    if len(non_zero_sales) < 5:
        return df_group
    q1, q3 = non_zero_sales.quantile(0.25), non_zero_sales.quantile(0.75)
    iqr = q3 - q1
    lower_bound, upper_bound = max(0, q1 - 1.5 * iqr), q3 + 1.5 * iqr
    df_group["ë§¤ì¶œìˆ˜ëŸ‰"] = np.clip(df_group["ë§¤ì¶œìˆ˜ëŸ‰"], lower_bound, upper_bound)
    return df_group


train = train.groupby("ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…", group_keys=False).apply(handle_outliers_iqr)
print("ì´ìƒì¹˜ ì²˜ë¦¬ ì™„ë£Œ.")

# 3.2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
sample = pd.read_csv(SAMPLE_FP)
tests = {}
for i in range(10):
    name = f"TEST_{i:02d}"
    df = pd.read_csv(TEST_DIR / f"{name}.csv")
    df["ì˜ì—…ì¼ìž"] = pd.to_datetime(df["ì˜ì—…ì¼ìž"])
    tests[name] = df

# ===== 4. íŠ¹ì§• ê³µí•™ (Feature Engineering) =====
print("íŠ¹ì§• ê³µí•™ ì‹œìž‘...")
le = LabelEncoder()
train["item_id"] = le.fit_transform(train["ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…"])


def make_date_feats(df):
    out = df.copy()
    out["year"], out["month"], out["day"], out["weekday"] = (
        out["ì˜ì—…ì¼ìž"].dt.year,
        out["ì˜ì—…ì¼ìž"].dt.month,
        out["ì˜ì—…ì¼ìž"].dt.day,
        out["ì˜ì—…ì¼ìž"].dt.weekday,
    )
    out["is_weekend"] = out["weekday"].isin([5, 6]).astype(int)
    out["month_sin"], out["month_cos"] = np.sin(
        2 * np.pi * out["month"] / 12.0
    ), np.cos(2 * np.pi * out["month"] / 12.0)
    out["wday_sin"], out["wday_cos"] = np.sin(2 * np.pi * out["weekday"] / 7.0), np.cos(
        2 * np.pi * out["weekday"] / 7.0
    )
    return out


train = make_date_feats(train)
train = train.sort_values(["item_id", "ì˜ì—…ì¼ìž"])

for lag in [1, 7, 14, 28]:
    train[f"lag{lag}"] = train.groupby("item_id")["ë§¤ì¶œìˆ˜ëŸ‰"].shift(lag)

g = train.groupby("item_id")["ë§¤ì¶œìˆ˜ëŸ‰"]
train["roll7_mean"], train["roll14_mean"], train["roll7_std"] = (
    g.shift(1).rolling(7).mean(),
    g.shift(1).rolling(14).mean(),
    g.shift(1).rolling(7).std(),
)
train = train.dropna()
print("íŠ¹ì§• ê³µí•™ ì™„ë£Œ.")

feature_cols = [
    "year",
    "month",
    "day",
    "weekday",
    "is_weekend",
    "month_sin",
    "month_cos",
    "wday_sin",
    "wday_cos",
    "item_id",
    "lag1",
    "lag7",
    "lag14",
    "lag28",
    "roll7_mean",
    "roll14_mean",
    "roll7_std",
]
categorical_feature = ["item_id"]
X, y = train[feature_cols], train["ë§¤ì¶œìˆ˜ëŸ‰"].astype(float)

# ===== 5. ëª¨ë¸ í•™ìŠµ =====
tscv = TimeSeriesSplit(n_splits=5)
# êµì°¨ ê²€ì¦ì˜ ë§ˆì§€ë§‰ foldë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ íšŸìˆ˜ë¥¼ ê²°ì •í•˜ê³  ìµœì¢… ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
tr_idx, va_idx = list(tscv.split(X))[-1]
X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]
y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]

# --- 5.1. LightGBM ëª¨ë¸ í•™ìŠµ ---
print("LightGBM ëª¨ë¸ í•™ìŠµ ì‹œìž‘...")
lgbm = lgb.LGBMRegressor(
    objective="regression_l1",  # MAEë¥¼ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ì— ì¢€ ë” ê°•ê±´í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    metric="rmse",
    n_estimators=5000,
    learning_rate=0.05,
    num_leaves=31,
    max_depth=-1,
    seed=42,
    n_jobs=-1,
    verbose=-1,
    colsample_bytree=0.8,
    subsample=0.8,
)
lgbm.fit(
    X_tr,
    y_tr,
    eval_set=[(X_va, y_va)],
    eval_metric="rmse",
    callbacks=[lgb.early_stopping(100, verbose=False)],
)
print("LightGBM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

# --- 5.2. CatBoost ëª¨ë¸ í•™ìŠµ ---
print("CatBoost ëª¨ë¸ í•™ìŠµ ì‹œìž‘...")
cat = cb.CatBoostRegressor(
    loss_function="RMSE",
    eval_metric="RMSE",
    iterations=5000,
    learning_rate=0.05,
    depth=8,
    random_seed=42,
    verbose=0,
    cat_features=categorical_feature,  # 'item_id'ê°€ ë²”ì£¼í˜• íŠ¹ì§•ìž„ì„ ëª…ì‹œ
)
cat.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], early_stopping_rounds=100, verbose=False)
print("CatBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

# ===== 6. ìž¬ê·€ ì˜ˆì¸¡ ë° ì•™ìƒë¸” =====
print("ìž¬ê·€ ì˜ˆì¸¡ ë° ì•™ìƒë¸” ì‹œìž‘...")
all_preds = []
full_history = train.copy()

for test_name, test_df in tests.items():
    test_df = test_df.copy()
    test_df["item_id"] = le.transform(test_df["ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…"])
    test_df = make_date_feats(test_df)

    history = pd.concat([full_history, test_df], ignore_index=True)
    history = history.sort_values(["item_id", "ì˜ì—…ì¼ìž"])

    last_date = test_df["ì˜ì—…ì¼ìž"].max()
    items = test_df["ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…"].unique()

    preds_rows = []
    current_date = last_date
    for step in range(1, 8):
        target_date = current_date + pd.Timedelta(days=1)
        frame = pd.DataFrame(
            {"ì˜ì—…ì¼ìž": np.repeat(target_date, len(items)), "ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…": items}
        )
        frame["item_id"] = le.transform(frame["ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…"])
        frame = make_date_feats(frame)

        temp_hist = history.copy()
        for lag in [1, 7, 14, 28]:
            lagged = temp_hist[["ì˜ì—…ì¼ìž", "item_id", "ë§¤ì¶œìˆ˜ëŸ‰"]].copy()
            lagged["ì˜ì—…ì¼ìž"] = lagged["ì˜ì—…ì¼ìž"] + pd.Timedelta(days=lag)
            frame = frame.merge(
                lagged.rename(columns={"ë§¤ì¶œìˆ˜ëŸ‰": f"lag{lag}"}),
                on=["ì˜ì—…ì¼ìž", "item_id"],
                how="left",
            )

        roll_base = temp_hist.sort_values(["item_id", "ì˜ì—…ì¼ìž"]).copy()
        gb = roll_base.groupby("item_id")["ë§¤ì¶œìˆ˜ëŸ‰"]
        roll_base["roll7_mean"] = gb.rolling(7).mean().reset_index(0, drop=True)
        roll_base["roll14_mean"] = gb.rolling(14).mean().reset_index(0, drop=True)
        roll_base["roll7_std"] = gb.rolling(7).std().reset_index(0, drop=True)
        roll_base["ì˜ì—…ì¼ìž"] = roll_base["ì˜ì—…ì¼ìž"] + pd.Timedelta(days=1)
        frame = frame.merge(
            roll_base[
                ["ì˜ì—…ì¼ìž", "item_id", "roll7_mean", "roll14_mean", "roll7_std"]
            ],
            on=["ì˜ì—…ì¼ìž", "item_id"],
            how="left",
        )

        frame[feature_cols] = frame[feature_cols].fillna(0)
        X_pred = frame[feature_cols]

        # --- ë‘ ëª¨ë¸ë¡œ ê°ê° ì˜ˆì¸¡ ---
        pred_lgbm = lgbm.predict(X_pred)
        pred_cat = cat.predict(X_pred)

        # --- ì˜ˆì¸¡ ê²°ê³¼ ì•™ìƒë¸” (ë‹¨ìˆœ í‰ê· ) ---
        yhat = (pred_lgbm + pred_cat) / 2.0
        yhat = np.clip(yhat, 0, None)  # ìŒìˆ˜ ì˜ˆì¸¡ ë°©ì§€
        frame["pred"] = yhat

        # ì•™ìƒë¸”ëœ ì˜ˆì¸¡ê°’ì„ historyì— ì¶”ê°€í•˜ì—¬ ë‹¤ìŒ ë‚  ì˜ˆì¸¡ì— ì‚¬ìš©
        add_hist = frame[["ì˜ì—…ì¼ìž", "item_id", "ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…", "pred"]].rename(
            columns={"pred": "ë§¤ì¶œìˆ˜ëŸ‰"}
        )
        history = pd.concat([history, add_hist], ignore_index=True)

        frame_out = frame[["ì˜ì—…ì¼ìž", "ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…", "pred"]].copy()
        frame_out["ì˜ì—…ì¼ìž"] = f"{test_name}+{step}ì¼"
        preds_rows.append(frame_out)

        current_date = target_date

    test_pred = pd.concat(preds_rows, ignore_index=True)
    wide = test_pred.pivot(index="ì˜ì—…ì¼ìž", columns="ì˜ì—…ìž¥ëª…_ë©”ë‰´ëª…", values="pred")
    all_preds.append(wide)

# ===== 7. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± =====
submission = pd.concat(all_preds)
submission = submission.reset_index().rename(columns={"index": "ì˜ì—…ì¼ìž"})
# sample.columnsì— ì—†ëŠ” ì»¬ëŸ¼ì´ submissionì— ìžˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬, sampleì— ìžˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
submission = submission[sample.columns]

out_path = BASE_DIR / "submission_lgbm_cat_ensemble.csv"
submission.to_csv(out_path, index=False, encoding="utf-8-sig")

print(f"âœ… ìµœì¢… ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ì €ìž¥ ì™„ë£Œ: {out_path}")
